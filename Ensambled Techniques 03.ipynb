{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e820b17b-057b-48cc-8629-a3ba18af1908",
   "metadata": {},
   "source": [
    "# Ensemble Techniques And Its Types-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c2d4f-7828-4a68-b38c-afc6d93e266c",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399573b9-4102-4096-9454-0605ed0c5093",
   "metadata": {},
   "source": [
    "# Q1. What is Random Forest Regressor ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d134227-4468-4a6d-93e2-606fa657bc8b",
   "metadata": {},
   "source": [
    "Random Forest Regressor is one of the ensamble techniques, which used the multiple individual models for predicting a gernalaizing outcome which is more accurate.These individual models are decision trees, in other words, Random forest regressor considers decision tree as their base learners. As it is already mentioned in it that it is used for classifiaction problems. However Random Forest Regressor also avilable in bagging for solving the regression problems. It is a alorithem of bagging technique which combine the raw sampling and feature sampling. Here the deep analysis is given , that how it works :\n",
    "\n",
    "  **Bootstrap Sampling :** Bootstrap sampling is used in the forest fire regressor algorithem for dividing the data into various samples. However these samples maybe repeated , which means , there should be the same samples of the data. It may also be possible that some of the data will remain after sampling. As bootstrap is selecting the data very randomly. This is also a cause of reduction of overfitting in a model. When data has been takin vary randomly as like bootstrap, the training data is not being too obssesed with the model and when a new data has come as a input then model should give a good accuracy in predictions.Random feature selection ensures that no single feature dominates the decision-making process.\n",
    "  \n",
    "  **Decision trees are the base Learners :** As we have learned that decision trees are the base learners in the Random Forest Classifier. Which is a big advnatge, which we get by using this ensamble technique.After sampling the data by bootstrap, various samples of the data from the dataset has been evolved. And each data sample is rewarded by a new decision tree. Each decision tree has trained the different sample of the data and capturing the noise of the data , by their own way. Because of this way of exicution the verience of the trainintg data should be low and peeformance of the model will increased.\n",
    "  \n",
    "  **Aggrigration Of the Outputs** : These decision trees, who ar used as individual model.For regression tasks, the predictions of individual trees are combined through averaging. Each tree in the forest predicts a continuous value, and the final prediction is often voating of these values.\n",
    "These aggrigration process is also helps us to make the model genralize.\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffea18a-2309-4194-94cf-8826fd038d1d",
   "metadata": {},
   "source": [
    "# Q2. How does Random Forest Regressor reduce the risk of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6053f5c3-0350-4489-8dd0-a9a54b6fda7b",
   "metadata": {},
   "source": [
    "Random Forest Regressor is an alogrithem of baging ensamble technique , which reduces the risk of overfitting. When the verience of the test data is too high and training data has too much fitted while training , then such overfitting issues ocuured. For reducing the risk of overfitting Random forest regressior has certain feature which shown below :\n",
    "\n",
    "  **Bootstrap Sampling :** Bootstarp sampling is basically divides the data into random samples of data for training the multiple. Each model takes a sample of data for their training. Because of this process the risk of the outliers will be reduced and impact of the noise in training data also be reduced by which overfiitting is ceased as possible.\n",
    "\n",
    "  **Feature Randomization :** As we discussed the bootstrap sampling selected the data samples very randomely in which some time some of the data whould be lift or some of the sample are reapted also.This helps in reducing the correlation between trees and ensures that no single feature dominates the decision-making process.\n",
    "  \n",
    "  **Multiple Trees Averaging  :** As we know that there are multiple decision trees for training the data samples individually. Each sample has their own decision tree for training. This averaging process helps to smooth out the predictions and reduce the impact of noise or outliers present in individual trees, leading to a more robust and generalized model.\n",
    "  \n",
    "  **Hyperparameter Tuning  :** There are many hyperperameters in the Forest Fire classifier , which helps us to control the complexity of the decision trees.By carefully tuning these hyperparameters, such as the maximum depth of the trees or the number of features considered for each split, one can find a balance between model complexity and generalization performance, reducing the risk of overfitting.\n",
    "  \n",
    "  \n",
    "  - accuracy of Training Data = Low Bias \n",
    "- accuracy of test data = high verience(over fitting)-->by RandomForest clf-->Low var (balnaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b13a18-4023-46e8-b08d-22c1eff4d957",
   "metadata": {},
   "source": [
    "# Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc370560-70aa-4087-a665-5223f78de1ff",
   "metadata": {},
   "source": [
    "In Random forest fire , there are sevral decision trees by which samples of the data are trained and giving their own outputs accordingly. So at the end , we have many outputs from multiple decision trees model but for predicting the pragmatic output we need a specific one. So in such a case we are using voating method. We are doing the vaoting of the various outrputs of decision trees models , the one output which is occured most of the time with respect to other, we should elect that output as our final output.\n",
    "\n",
    "   For example , there are 9 decision trees used in our Random forest regressior, for a binary classification they giving their individual predictions. In which 6 models has predict 0 or \"NO\" and rest 3 has predict 1 or \"Yes\" as a output . Then our final decision is 0 or \"No\" by the voating. This aggrigation technique is also helps to reduce overfitting as we have discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01216c47-7aff-462b-bd02-19a5b7778cb9",
   "metadata": {},
   "source": [
    "# Q4. What are the hyperparameters of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee42e5-e850-4232-9841-1692f3fe364a",
   "metadata": {},
   "source": [
    "Random Forest Regressor has several hyperparameters that can be tuned to optimize the model's performance. Which explained below :\n",
    "\n",
    "**n_estimators:**\n",
    "\n",
    "- Definition: The number of trees in the forest.\n",
    "- Default: 100\n",
    "- Recommendation: Higher values may lead to better performance, but also increase computational cost.\n",
    "\n",
    "**max_depth**\n",
    "\n",
    "- Definition: The maximum depth of each decision tree in the forest.\n",
    "- Default: None \n",
    "- Recommendation: Controlling the depth helps prevent overfitting. \n",
    "\n",
    "**min_samples_split:**\n",
    "\n",
    "- Definition: The minimum number of samples required to split an internal node.\n",
    "- Default: 2\n",
    "- Recommendation: Increasing this value can lead to a more robust model by preventing the creation of nodes with very few samples.\n",
    "\n",
    "**min_samples_leaf:**\n",
    "\n",
    "- Definition: The minimum number of samples required to be at a leaf node.\n",
    "- Default: 1\n",
    "- Recommendation: Increasing this value can help smooth the model and prevent it from being too sensitive to noise in the training data.\n",
    "\n",
    "**max_features:**\n",
    "\n",
    "- Definition: The number of features to consider when looking for the best split.\n",
    "- Default: \"auto\" \n",
    "- Recommendation: Adjusting this parameter can add more randomness to the model and prevent individual features from dominating the decision-making process.\n",
    "\n",
    "**bootstrap:**\n",
    "\n",
    "- Definition: Whether to use bootstrap samples when building trees.\n",
    "- Default: True\n",
    "- Recommendation: Setting this to False disables bootstrapping, meaning that each tree is trained on the entire dataset. In some cases, this may lead to overfitting, so it's often left at the default (True).\n",
    "\n",
    "\n",
    "     We can find the best peremeters as our situation of data by the hyper perameter tuning and cross validation . If the data is not too big then we should use grid search cv and if the dataset is to large then we should go for randomized search cv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411965c5-6734-4b14-bbf7-1a556248d1b9",
   "metadata": {},
   "source": [
    "# Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6546ae-6151-4aff-862a-b2007639ed80",
   "metadata": {},
   "source": [
    "Random forest regressior and Decision tree regressor both are the meachine learning algorithems which are used for solving the regreesion problems. Both are tained theirself with a certain data and giving the outputs for new data according to trained data. Randome Forest regressor is a bagging technique which useses the multiple decision tree regressors for their final prediction while decision trees are worked individually. There are some diffrences shown below :\n",
    "\n",
    "**Different realm :** Randome Forest fire has a wide realme than dicision tree regressor as it is included the sevral decision trees in itself. We can say that Random forest regressior is an essense of the dicision trees.\n",
    "\n",
    "**Performance :** As we know that Random forest fire has been made up with the decision trees , it meant that the performance of the random forest fire is better than decision tree regressior as in forest fire , there are the potenitial of many dicison trees while decison tree regressor works individually.\n",
    "\n",
    "**Overfitting :** Ofcourse , Random forest fire is might good then decision tree regresor in the case of overfitting. Beacuse it has many features like Bootstrap sampling , aggrigation of predictions. Which can not be the data to be too much obssesed with the training data and also take a wide sight on outliers in order to reduce their impact. while in dicision tree there are no such any speciality.\n",
    "\n",
    "**Computetional Cost** : In such a case decision tree favorable beacuse Random forest fire is too computentionaly expensive beacuse there wide range of processings. while decision tree works well in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062498b0-0dbd-4994-95c0-ead5dc3b91f8",
   "metadata": {},
   "source": [
    "# Q6. What are the advantages and disadvantages of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b9e40-bd97-485c-9884-f4271ba30e1b",
   "metadata": {},
   "source": [
    "Randome Forest fire is a popular baging technique of ensamble , which we are use to solve the classification problems and our aim is to using this regressor is always be get a got accuracy of model and get rid from the impact of outliers. However, there are some advantages and disadvantages to use such techniques, which we are disscused below ;\n",
    "\n",
    "\n",
    "                                          Advantages\n",
    "                                    \n",
    "  **High performance :** Forest fire regressor is the aggrigation of the potential of sevral decison trees. So it is easy to know that it accurcy will always be high.\n",
    "  \n",
    "  **Rebust to outliers :** This algorithem is reduces the impact of the outliers and overfitting as web had discussed that it has many such features as bootstrap samplng , aggrigation techniques, genraliaztion by sevral decision trees. These factore are cease to be overfit the model.\n",
    "  \n",
    "  **Handles Non-linearity:** Random Forests can capture non-linear relationships and interactions between features, making them suitable for a wide range of data types and structures.\n",
    "\n",
    "  **Feature Importance:** Random Forests provide a feature importance score, which can help identify the most influential features in the model. This information is valuable for feature selection and understanding the dataset.\n",
    "\n",
    "   **Handles Missing Values:** Random Forests can handle missing values in the dataset, and they do not require imputation of missing values before training.\n",
    "\n",
    "\n",
    "                                           Disadvantages\n",
    "                                           \n",
    "                                           \n",
    "**Computetionally expensive :** As we have already discussed sevral times that the Random forest fire algorithem is computentionally expensive beacuse of the wide ranges of opreations as it works..ie..trained the data by sevral models takes too much time.\n",
    "\n",
    "**Lack of Interpretability:** Random Forests are often considered as \"black-box\" models, meaning they are less interpretable compared to simpler models like linear regression. \n",
    "\n",
    "**Training Time:** Training a Random Forest can be time-consuming, especially for large datasets. However, this disadvantage can be partially mitigated by parallelization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540b208-9872-4834-9a06-b79811149740",
   "metadata": {},
   "source": [
    "# Q7. What is the output of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197980bc-bf08-49eb-8b76-5003b729c337",
   "metadata": {},
   "source": [
    "The output of a Random Forest Regressor is always be a numeical value but assembled with a classification varible. for example in case of binary there \"Yes \" can be denoted by 1 while \"No\" is 0. Here we see, we do we reach till output in Random Forest Regressor\n",
    "\n",
    "- Dataset ---> (By bootstrap sampling) ---> Divides in multiple sample ---> each sample trained with a decison tree ---> each decision tree gives their predictions ---> Agreegating all predictions by vaoting ---> Get final Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c018461-a7f1-423d-9c39-0f618d46cd51",
   "metadata": {},
   "source": [
    "# Q8. Can Random Forest Regressor be used for classification tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a3355-9b28-4def-83e8-fab935f596df",
   "metadata": {},
   "source": [
    " Random Forest Regressor is usually desined for regression problems, which deals with continues values and giving the output in continues formate. In the bagging technique there is an another alogirthem which is specially desined for classification problems but in some of the case random forest regressor can be used for classificatio problems. \n",
    " In case of binary when data is in a formate of 0 and 1 it can be solved by the regressor as we can also use the encoding techniques to perform this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
